{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2aec56c-3c2f-41db-8f37-e99ce3ffefed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.9.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (0.24.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.24.1\n",
      "    Uninstalling accelerate-0.24.1:\n",
      "      Successfully uninstalled accelerate-0.24.1\n",
      "Successfully installed accelerate-1.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e752c89b-65cf-4c20-8727-e04ff00714ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-1b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /usr/local/lib/python3.11/dist-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-10-27 01:19:45.951466: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-27 01:19:45.951539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-27 01:19:45.953087: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-27 01:19:45.961253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-27 01:19:47.250840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda!\n",
      "Model size: ~1.86 GB\n",
      "Testing LLM...\n",
      "\n",
      "================================================================================\n",
      "Raw Response (7.95s):\n",
      "================================================================================\n",
      " The Matrix (1999)\n",
      "2. Blade Runner (1982)\n",
      "3. Interstellar (2014)\n",
      "4. Fifth Element (1997)\n",
      "5. Dark City (1998)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Extracted Movies:\n",
      "================================================================================\n",
      "1. The Matrix (1999)\n",
      "2. Blade Runner (1982)\n",
      "3. Interstellar (2014)\n",
      "4. Fifth Element (1997)\n",
      "5. Dark City (1998)\n",
      "\n",
      "================================================================================\n",
      "Testing extraction with descriptions:\n",
      "================================================================================\n",
      "Input:\n",
      "1. Blade Runner 2049: A visually stunning sequel\n",
      "2. Inception (2010) - Mind-bending thriller\n",
      "3. The Matrix Reloaded\n",
      "4.  Arrival  –  Sci-fi drama about communication\n",
      "5. Interstellar (2014)\n",
      "\n",
      "Extracted:\n",
      "1. Blade Runner 2049\n",
      "2. Inception (2010)\n",
      "3. The Matrix Reloaded\n",
      "4. Arrival\n",
      "5. Interstellar (2014)\n"
     ]
    }
   ],
   "source": [
    "# llm_recommender.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "\n",
    "class LLMRecommender:\n",
    "    \"\"\"Wrapper genérico para cualquier LLM de HuggingFace\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = 'cuda', \n",
    "                 load_in_8bit: bool = False, max_memory_gb: int = 15):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: nombre del modelo en HuggingFace \n",
    "            device: 'cuda' o 'cpu'\n",
    "            load_in_8bit: si usar cuantización 8-bit\n",
    "            max_memory_gb: GB máximos a usar\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        \n",
    "        # Cargar tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Configurar pad token si no existe\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Cargar modelo con cuantización si se solicita\n",
    "        if load_in_8bit:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                max_memory={0: f\"{max_memory_gb}GB\"}\n",
    "            )\n",
    "        else:\n",
    "            dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=\"auto\",\n",
    "                max_memory={0: f\"{max_memory_gb}GB\"}\n",
    "            )\n",
    "        \n",
    "        self.model.eval()  # modo evaluación\n",
    "        print(f\"Model loaded successfully on {self.device}!\")\n",
    "        print(f\"Model size: ~{self.get_model_size_gb():.2f} GB\")\n",
    "    \n",
    "    def get_model_size_gb(self) -> float:\n",
    "        \"\"\"Calcula el tamaño del modelo en GB\"\"\"\n",
    "        param_size = sum(p.nelement() * p.element_size() for p in self.model.parameters())\n",
    "        buffer_size = sum(b.nelement() * b.element_size() for b in self.model.buffers())\n",
    "        return (param_size + buffer_size) / (1024**3)\n",
    "    \n",
    "    def generate_recommendations(self, context: str, prompt_template, \n",
    "                                max_new_tokens: int = 250,\n",
    "                                temperature: float = 0.7) -> tuple:\n",
    "        \"\"\"\n",
    "        Genera recomendaciones dado un contexto\n",
    "        \n",
    "        Args:\n",
    "            context: string con el contexto de la conversación\n",
    "            prompt_template: función que formatea el prompt\n",
    "            max_new_tokens: máximo de tokens a generar\n",
    "            temperature: temperatura para sampling\n",
    "            \n",
    "        Returns:\n",
    "            response: string con la respuesta del modelo\n",
    "            latency: tiempo de generación en segundos\n",
    "        \"\"\"\n",
    "        # Formatear prompt\n",
    "        prompt = prompt_template(context)\n",
    "        \n",
    "        # Tokenizar\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048  # limitar contexto para evitar OOM\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generar\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True if temperature > 0 else False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,  # evitar repetición\n",
    "                top_p=0.9,  # nucleus sampling\n",
    "            )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Decodificar solo la parte nueva\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][input_length:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return response, latency\n",
    "    \n",
    "    def extract_movie_titles(self, response: str, max_recommendations: int = 10) -> list:\n",
    "        \"\"\"\n",
    "        Extrae títulos de películas de la respuesta del modelo\n",
    "        VERSIÓN MEJORADA: separa título de descripción\n",
    "        \n",
    "        Args:\n",
    "            response: string con la respuesta del modelo\n",
    "            max_recommendations: máximo de recomendaciones a retornar\n",
    "            \n",
    "        Returns:\n",
    "            list de strings con títulos de películas (solo los títulos)\n",
    "        \"\"\"\n",
    "        movies = []\n",
    "        lines = response.strip().split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Saltar líneas vacías o muy cortas\n",
    "            if len(line) < 3:\n",
    "                continue\n",
    "            \n",
    "            # Saltar líneas que no parecen ser recomendaciones\n",
    "            skip_phrases = ['here are', 'based on', 'you might', 'i recommend', \n",
    "                           'these are', 'recommendation', 'user:', 'assistant:', \n",
    "                           'given this', 'conversation']\n",
    "            if any(phrase in line.lower()[:50] for phrase in skip_phrases):\n",
    "                continue\n",
    "            \n",
    "            # Remover numeración común: \"1.\", \"1)\", \"- \", etc.\n",
    "            line = re.sub(r'^[\\d\\-\\.\\)\\*\\•\\s]+', '', line)\n",
    "            \n",
    "            # NUEVO: Separar título de descripción\n",
    "            # Buscar el primer separador: \":\", \"-\", \"–\", o dos espacios\n",
    "            separators = [':', ' -', ' –', '  ']\n",
    "            title = line\n",
    "            \n",
    "            for sep in separators:\n",
    "                if sep in line:\n",
    "                    title = line.split(sep)[0].strip()\n",
    "                    break\n",
    "            \n",
    "            # Remover comillas si las hay\n",
    "            title = title.strip('\"\\'')\n",
    "            \n",
    "            # Limpiar caracteres extraños al final\n",
    "            title = re.sub(r'[:\\-–,\\.]$', '', title).strip()\n",
    "            \n",
    "            # Filtros de calidad\n",
    "            if 3 < len(title) < 100:  # longitud razonable\n",
    "                movies.append(title)\n",
    "            \n",
    "            if len(movies) >= max_recommendations:\n",
    "                break\n",
    "        \n",
    "        return movies[:max_recommendations]\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Limpia memoria GPU\"\"\"\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Test mejorado\n",
    "if __name__ == \"__main__\":\n",
    "    llm = LLMRecommender(\"google/gemma-3-1b-it\", load_in_8bit=False)\n",
    "    \n",
    "    test_context = \"\"\"User: I love action movies with great special effects like \"The Matrix (1999)\"\n",
    "Recommender: What kind of action movies do you prefer?\n",
    "User: Sci-fi action with deep philosophical themes.\"\"\"\n",
    "    \n",
    "    # Prompt más restrictivo\n",
    "    def simple_prompt(context):\n",
    "        return f\"\"\"Given this conversation about movies, recommend exactly 5 movies.\n",
    "\n",
    "Conversation:\n",
    "{context}\n",
    "\n",
    "Output ONLY a numbered list of movie titles with years. No descriptions or explanations.\n",
    "\n",
    "Recommendations:\n",
    "1.\"\"\"\n",
    "    \n",
    "    print(\"Testing LLM...\")\n",
    "    response, latency = llm.generate_recommendations(\n",
    "        test_context, \n",
    "        simple_prompt,\n",
    "        max_new_tokens=150,  # menos tokens\n",
    "        temperature=0.3  # más determinístico\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Raw Response ({latency:.2f}s):\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(response)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Extracted Movies:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    movies = llm.extract_movie_titles(response)\n",
    "    for i, movie in enumerate(movies, 1):\n",
    "        print(f\"{i}. {movie}\")\n",
    "    \n",
    "    # Test de extracción con texto complejo\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Testing extraction with descriptions:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    test_response = \"\"\"1. Blade Runner 2049: A visually stunning sequel\n",
    "2. Inception (2010) - Mind-bending thriller\n",
    "3. The Matrix Reloaded\n",
    "4.  Arrival  –  Sci-fi drama about communication\n",
    "5. Interstellar (2014)\"\"\"\n",
    "    \n",
    "    extracted = llm.extract_movie_titles(test_response)\n",
    "    print(\"Input:\")\n",
    "    print(test_response)\n",
    "    print(\"\\nExtracted:\")\n",
    "    for i, movie in enumerate(extracted, 1):\n",
    "        print(f\"{i}. {movie}\")\n",
    "    \n",
    "    llm.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e985d99d-9872-4b01-a8aa-b8eac72295e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prompts:\n",
      "\n",
      "=== ZERO_SHOT ===\n",
      "You are a movie recommendation expert. Given the following conversation between a user seeking movie recommendations and a recommender, suggest 10 movies that the user would enjoy based on their preferences.\n",
      "\n",
      "Conversation:\n",
      "User: I love action movies with great special effects like \"The Matrix (1999)...\n",
      "\n",
      "=== FEW_SHOT ===\n",
      "Here are some examples:\n",
      "\n",
      "Example 1:\n",
      "User: I love action movies with great special effects like \"The Matrix (1999)\"\n",
      "Recommender: What aspects do you enjoy most?\n",
      "User: The sci-fi elements and philosophical themes\n",
      "\n",
      "Recommendations:\n",
      "1. Inception (2010)\n",
      "2. Blade Runner (1982)\n",
      "3. The Prestige (2006)\n",
      "4. Mi...\n",
      "\n",
      "=== CHAIN_OF_THOUGHT ===\n",
      "You are a movie recommendation expert. Analyze the following conversation and recommend movies.\n",
      "\n",
      "Conversation:\n",
      "User: I love action movies with great special effects like \"The Matrix (1999)\"\n",
      "Recommender: What kind of action movies do you prefer?\n",
      "User: Sci-fi action with deep philosophical themes.\n",
      "\n",
      "Fi...\n",
      "\n",
      "=== ROLE_BASED ===\n",
      "You are an experienced movie critic and recommendation specialist with deep knowledge of cinema history, genres, and audience preferences. A user is seeking movie recommendations based on this conversation:\n",
      "\n",
      "Conversation:\n",
      "User: I love action movies with great special effects like \"The Matrix (1999)\"...\n",
      "\n",
      "=== STRUCTURED ===\n",
      "Given this movie conversation, provide 10 movie recommendations.\n",
      "\n",
      "Conversation:\n",
      "User: I love action movies with great special effects like \"The Matrix (1999)\"\n",
      "Recommender: What kind of action movies do you prefer?\n",
      "User: Sci-fi action with deep philosophical themes.\n",
      "\n",
      "Output format - provide exactly 1...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompts.py\n",
    "\n",
    "def zero_shot_prompt(context):\n",
    "    \"\"\"Prompt básico zero-shot\"\"\"\n",
    "    return f\"\"\"You are a movie recommendation expert. Given the following conversation between a user seeking movie recommendations and a recommender, suggest 10 movies that the user would enjoy based on their preferences.\n",
    "\n",
    "Conversation:\n",
    "{context}\n",
    "\n",
    "Provide exactly 10 movie recommendations as a numbered list. Output ONLY the movie titles with their release year in parentheses, one per line, nothing else.\n",
    "\n",
    "Recommendations:\n",
    "1.\"\"\"\n",
    "\n",
    "def few_shot_prompt(context):\n",
    "    \"\"\"Prompt con ejemplos (few-shot)\"\"\"\n",
    "    examples = \"\"\"Here are some examples:\n",
    "\n",
    "Example 1:\n",
    "User: I love action movies with great special effects like \"The Matrix (1999)\"\n",
    "Recommender: What aspects do you enjoy most?\n",
    "User: The sci-fi elements and philosophical themes\n",
    "\n",
    "Recommendations:\n",
    "1. Inception (2010)\n",
    "2. Blade Runner (1982)\n",
    "3. The Prestige (2006)\n",
    "4. Minority Report (2002)\n",
    "5. Total Recall (1990)\n",
    "\n",
    "Example 2:\n",
    "User: I enjoy romantic comedies like \"When Harry Met Sally (1989)\"\n",
    "Recommender: Do you prefer classic or modern rom-coms?\n",
    "User: I like both, but prefer ones with witty dialogue\n",
    "\n",
    "Recommendations:\n",
    "1. Sleepless in Seattle (1993)\n",
    "2. You've Got Mail (1998)\n",
    "3. Notting Hill (1999)\n",
    "4. The Proposal (2009)\n",
    "5. Crazy, Stupid, Love (2011)\n",
    "\n",
    "\"\"\"\n",
    "    return f\"\"\"{examples}\n",
    "\n",
    "Now provide recommendations for this conversation:\n",
    "\n",
    "Conversation:\n",
    "{context}\n",
    "\n",
    "Provide exactly 10 movie recommendations as a numbered list. Output ONLY the movie titles with their release year in parentheses, one per line.\n",
    "\n",
    "Recommendations:\n",
    "1.\"\"\"\n",
    "\n",
    "def cot_prompt(context):\n",
    "    \"\"\"Chain-of-thought prompt\"\"\"\n",
    "    return f\"\"\"You are a movie recommendation expert. Analyze the following conversation and recommend movies.\n",
    "\n",
    "Conversation:\n",
    "{context}\n",
    "\n",
    "First, analyze what the user likes (genres, themes, styles). Then provide 10 movie recommendations.\n",
    "\n",
    "Analysis of user preferences:\n",
    "- Genre preferences: [identify genres mentioned]\n",
    "- Specific movies liked: [list movies user responded positively to]\n",
    "- Themes/styles: [identify patterns]\n",
    "\n",
    "Based on this analysis, here are 10 movie recommendations:\n",
    "1.\"\"\"\n",
    "\n",
    "def role_prompt(context):\n",
    "    \"\"\"Role-based prompt with specific persona\"\"\"\n",
    "    return f\"\"\"You are an experienced movie critic and recommendation specialist with deep knowledge of cinema history, genres, and audience preferences. A user is seeking movie recommendations based on this conversation:\n",
    "\n",
    "Conversation:\n",
    "{context}\n",
    "\n",
    "As an expert recommender, provide 10 carefully selected movie recommendations that match the user's taste. Consider genre, themes, era, and style.\n",
    "\n",
    "Your 10 recommendations:\n",
    "1.\"\"\"\n",
    "\n",
    "def structured_prompt(context):\n",
    "    \"\"\"Prompt pidiendo output estructurado\"\"\"\n",
    "    return f\"\"\"Given this movie conversation, provide 10 movie recommendations.\n",
    "\n",
    "Conversation:\n",
    "{context}\n",
    "\n",
    "Output format - provide exactly 10 movies in this format:\n",
    "1. [Movie Title] ([Year])\n",
    "2. [Movie Title] ([Year])\n",
    "...\n",
    "\n",
    "Recommendations:\n",
    "1.\"\"\"\n",
    "\n",
    "# Diccionario para fácil acceso\n",
    "PROMPT_STRATEGIES = {\n",
    "    'zero_shot': zero_shot_prompt,\n",
    "    'few_shot': few_shot_prompt,\n",
    "    'chain_of_thought': cot_prompt,\n",
    "    'role_based': role_prompt,\n",
    "    'structured': structured_prompt\n",
    "}\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    test_context = \"\"\"User: I love action movies with great special effects like \"The Matrix (1999)\"\n",
    "Recommender: What kind of action movies do you prefer?\n",
    "User: Sci-fi action with deep philosophical themes.\"\"\"\n",
    "    \n",
    "    print(\"Testing prompts:\\n\")\n",
    "    for name, prompt_fn in PROMPT_STRATEGIES.items():\n",
    "        print(f\"=== {name.upper()} ===\")\n",
    "        print(prompt_fn(test_context)[:300] + \"...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed34001-acb5-40ba-b016-6ede58e47a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing evaluator:\n",
      "Recommended: ['The Matrix (1999)', 'Inception (2010)', 'Blade Runner (1982)', 'The Prestige (2006)']\n",
      "Ground truth: ['The Matrix (1999)', 'Inception (2010)', 'Interstellar (2014)']\n",
      "\n",
      "recall@3: 0.6667\n",
      "precision@3: 0.6667\n",
      "ndcg@3: 0.7654\n",
      "hit_rate@3: 1.0000\n",
      "recall@5: 0.6667\n",
      "precision@5: 0.5000\n",
      "ndcg@5: 0.7654\n",
      "hit_rate@5: 1.0000\n",
      "mrr: 1.0000\n",
      "\n",
      "\n",
      "Testing fuzzy matching:\n",
      "The Matrix (1999) <-> Matrix (1999): 1.000\n",
      "The Lord of the Rings <-> Lord of Rings: 1.000\n",
      "Star Wars <-> Star Wars: A New Hope: 0.621\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class RecommendationEvaluator:\n",
    "    \"\"\"Calcula métricas de evaluación para recomendaciones\"\"\"\n",
    "    \n",
    "    def __init__(self, fuzzy_match_threshold: float = 0.8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fuzzy_match_threshold: umbral de similitud para considerar match (0-1)\n",
    "        \"\"\"\n",
    "        self.fuzzy_match_threshold = fuzzy_match_threshold\n",
    "    \n",
    "    def normalize_title(self, title: str) -> str:\n",
    "        \"\"\"Normaliza un título de película para comparación\"\"\"\n",
    "        # Minúsculas\n",
    "        title = title.lower()\n",
    "        # Remover año si está presente\n",
    "        import re\n",
    "        title = re.sub(r'\\s*\\(\\d{4}\\)\\s*', '', title)\n",
    "        # Remover espacios extras\n",
    "        title = ' '.join(title.split())\n",
    "        # Remover puntuación común\n",
    "        title = title.replace(',', '').replace('.', '').replace(':', '').replace('the ', '')\n",
    "        return title.strip()\n",
    "    \n",
    "    def fuzzy_match(self, title1: str, title2: str) -> float:\n",
    "        \"\"\"Calcula similitud entre dos títulos (0-1)\"\"\"\n",
    "        norm1 = self.normalize_title(title1)\n",
    "        norm2 = self.normalize_title(title2)\n",
    "        return SequenceMatcher(None, norm1, norm2).ratio()\n",
    "    \n",
    "    def match_titles(self, recommended: List[str], ground_truth: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Encuentra matches entre recomendaciones y ground truth usando fuzzy matching\n",
    "        \n",
    "        Returns:\n",
    "            Lista de títulos de ground_truth que fueron recomendados\n",
    "        \"\"\"\n",
    "        matched = []\n",
    "        \n",
    "        for gt_title in ground_truth:\n",
    "            for rec_title in recommended:\n",
    "                similarity = self.fuzzy_match(gt_title, rec_title)\n",
    "                if similarity >= self.fuzzy_match_threshold:\n",
    "                    matched.append(gt_title)\n",
    "                    break  # ya encontramos match para este gt_title\n",
    "        \n",
    "        return matched\n",
    "    \n",
    "    def recall_at_k(self, recommended: List[str], ground_truth: List[str], k: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Recall@K: proporción de items relevantes que fueron recomendados\n",
    "        \"\"\"\n",
    "        if len(ground_truth) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        recommended_k = recommended[:k]\n",
    "        matched = self.match_titles(recommended_k, ground_truth)\n",
    "        \n",
    "        return len(matched) / len(ground_truth)\n",
    "    \n",
    "    def precision_at_k(self, recommended: List[str], ground_truth: List[str], k: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Precision@K: proporción de items recomendados que son relevantes\n",
    "        \"\"\"\n",
    "        if len(recommended) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        recommended_k = recommended[:k]\n",
    "        matched = self.match_titles(recommended_k, ground_truth)\n",
    "        \n",
    "        return len(matched) / min(len(recommended_k), k)\n",
    "    \n",
    "    def ndcg_at_k(self, recommended: List[str], ground_truth: List[str], k: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        NDCG@K: Normalized Discounted Cumulative Gain\n",
    "        \"\"\"\n",
    "        recommended_k = recommended[:k]\n",
    "        \n",
    "        # DCG\n",
    "        dcg = 0.0\n",
    "        for i, rec_title in enumerate(recommended_k):\n",
    "            # Verificar si este título hace match con alguno del ground truth\n",
    "            for gt_title in ground_truth:\n",
    "                if self.fuzzy_match(rec_title, gt_title) >= self.fuzzy_match_threshold:\n",
    "                    dcg += 1.0 / np.log2(i + 2)  # +2 porque empieza en 0\n",
    "                    break\n",
    "        \n",
    "        # IDCG (ideal DCG)\n",
    "        idcg = 0.0\n",
    "        for i in range(min(len(ground_truth), k)):\n",
    "            idcg += 1.0 / np.log2(i + 2)\n",
    "        \n",
    "        if idcg == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dcg / idcg\n",
    "    \n",
    "    def hit_rate_at_k(self, recommended: List[str], ground_truth: List[str], k: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Hit Rate@K: 1 si al menos 1 item relevante está en top-K, 0 si no\n",
    "        \"\"\"\n",
    "        recommended_k = recommended[:k]\n",
    "        matched = self.match_titles(recommended_k, ground_truth)\n",
    "        return 1.0 if len(matched) > 0 else 0.0\n",
    "    \n",
    "    def mrr(self, recommended: List[str], ground_truth: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Mean Reciprocal Rank: 1/rank del primer item relevante\n",
    "        \"\"\"\n",
    "        for i, rec_title in enumerate(recommended):\n",
    "            for gt_title in ground_truth:\n",
    "                if self.fuzzy_match(rec_title, gt_title) >= self.fuzzy_match_threshold:\n",
    "                    return 1.0 / (i + 1)\n",
    "        return 0.0\n",
    "    \n",
    "    def evaluate_all(self, recommended: List[str], ground_truth: List[str], \n",
    "                     k_values: List[int] = [5, 10]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calcula todas las métricas\n",
    "        \n",
    "        Returns:\n",
    "            dict con todas las métricas\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for k in k_values:\n",
    "            metrics[f'recall@{k}'] = self.recall_at_k(recommended, ground_truth, k)\n",
    "            metrics[f'precision@{k}'] = self.precision_at_k(recommended, ground_truth, k)\n",
    "            metrics[f'ndcg@{k}'] = self.ndcg_at_k(recommended, ground_truth, k)\n",
    "            metrics[f'hit_rate@{k}'] = self.hit_rate_at_k(recommended, ground_truth, k)\n",
    "        \n",
    "        metrics['mrr'] = self.mrr(recommended, ground_truth)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = RecommendationEvaluator()\n",
    "    \n",
    "    # Test con títulos exactos\n",
    "    recommended = [\n",
    "        \"The Matrix (1999)\",\n",
    "        \"Inception (2010)\",\n",
    "        \"Blade Runner (1982)\",\n",
    "        \"The Prestige (2006)\"\n",
    "    ]\n",
    "    \n",
    "    ground_truth = [\n",
    "        \"The Matrix (1999)\",\n",
    "        \"Inception (2010)\",\n",
    "        \"Interstellar (2014)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing evaluator:\")\n",
    "    print(f\"Recommended: {recommended}\")\n",
    "    print(f\"Ground truth: {ground_truth}\")\n",
    "    print()\n",
    "    \n",
    "    metrics = evaluator.evaluate_all(recommended, ground_truth, k_values=[3, 5])\n",
    "    \n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Test fuzzy matching\n",
    "    print(\"\\n\\nTesting fuzzy matching:\")\n",
    "    test_pairs = [\n",
    "        (\"The Matrix (1999)\", \"Matrix (1999)\"),\n",
    "        (\"The Lord of the Rings\", \"Lord of Rings\"),\n",
    "        (\"Star Wars\", \"Star Wars: A New Hope\"),\n",
    "    ]\n",
    "    \n",
    "    for t1, t2 in test_pairs:\n",
    "        similarity = evaluator.fuzzy_match(t1, t2)\n",
    "        print(f\"{t1} <-> {t2}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11f6d65-19ef-45f8-afe5-1f511861320c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from ./test_data.jsonl...\n",
      "Loaded 1342 conversations\n",
      "Prepared 5 samples with ground truth (min=1)\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1 - DETAILED\n",
      "================================================================================\n",
      "Conversation ID: 20001\n",
      "Total messages: 15\n",
      "Context messages: 10\n",
      "\n",
      "Context:\n",
      "User: Hi I am looking for a movie like \"Super Troopers (2001)\"\n",
      "Recommender: You should watch \"Police Academy  (1984)\"\n",
      "User: Is that a great one? I have never seen it. I have seen \"American Pie \"\n",
      "User: I mean \"American Pie  (1999)\"\n",
      "Recommender: Yes \"Police Academy  (1984)\" is very funny and so is \"Police Academy 2: Their First Assignment (1985)\"\n",
      "User: It sounds like I need to check them out\n",
      "Recommender: yes you will enjoy them\n",
      "User: I appreciate your time. I will need to check those out. Are there any others you would recommend?\n",
      "Recommender: yes \"Lethal Weapon (1987)\"\n",
      "User: Thank you i will watch that too\n",
      "\n",
      "Ground Truth: ['48 Hrs. (1982)', 'Beverly Hills Cop (1984)']\n",
      "\n",
      "All movies mentioned: ['Super Troopers (2001)', 'Beverly Hills Cop (1984)', 'Police Academy  (1984)', 'American Pie  (1999)', 'American Pie ', '48 Hrs. (1982)', 'Police Academy 2: Their First Assignment (1985)', 'Lethal Weapon (1987)']\n",
      "\n",
      "================================================================================\n",
      "STATISTICS\n",
      "================================================================================\n",
      "Total samples: 5\n",
      "Avg ground truth size: 1.80\n",
      "Min/Max ground truth: 1/4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "class ReDiALDataset:\n",
    "    \"\"\"Clase para cargar y procesar el dataset ReDiAL\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, split: str = 'test'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: ruta al directorio con los archivos (ej: './redial_data' o '.')\n",
    "            split: 'train', 'validation', o 'test'\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.split = split\n",
    "        self.conversations = []\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Carga las conversaciones del archivo correspondiente\"\"\"\n",
    "        # Mapeo de nombres\n",
    "        file_mapping = {\n",
    "            'train': 'train_data.jsonl',\n",
    "            'validation': 'valid_data.jsonl',\n",
    "            'test': 'test_data.jsonl'\n",
    "        }\n",
    "        \n",
    "        file_name = file_mapping[self.split]\n",
    "        \n",
    "        # Intentar con y sin subdirectorio\n",
    "        try:\n",
    "            file_path = f\"{self.data_path}/{file_name}\"\n",
    "            with open(file_path, 'r') as f:\n",
    "                test_line = f.readline()\n",
    "        except FileNotFoundError:\n",
    "            file_path = file_name\n",
    "        \n",
    "        print(f\"Loading {self.split} data from {file_path}...\")\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                conv = json.loads(line)\n",
    "                self.conversations.append(conv)\n",
    "        \n",
    "        print(f\"Loaded {len(self.conversations)} conversations\")\n",
    "    \n",
    "    def format_conversation_context(self, conversation: Dict,\n",
    "                                    n_messages: int) -> str:\n",
    "        \"\"\"\n",
    "        Formatea los mensajes en un string legible para el LLM\n",
    "        \n",
    "        Args:\n",
    "            conversation: conversación completa con metadata\n",
    "            n_messages: cuántos mensajes incluir\n",
    "            \n",
    "        Returns:\n",
    "            string con la conversación formateada\n",
    "        \"\"\"\n",
    "        messages = conversation['messages']\n",
    "        movie_mentions = conversation['movieMentions']\n",
    "        \n",
    "        # Identificar quién es el seeker (iniciador) y quién el recommender\n",
    "        seeker_id = conversation['initiatorWorkerId']\n",
    "        recommender_id = conversation['respondentWorkerId']\n",
    "        \n",
    "        formatted = []\n",
    "        \n",
    "        for msg in messages[:n_messages]:\n",
    "            # Mapear correctamente según el workerId\n",
    "            sender_id = msg['senderWorkerId']\n",
    "            \n",
    "            if sender_id == seeker_id:\n",
    "                sender = \"User\"  # El que busca recomendaciones\n",
    "            elif sender_id == recommender_id:\n",
    "                sender = \"Recommender\"  # El que recomienda\n",
    "            else:\n",
    "                sender = f\"Speaker{sender_id}\"  # fallback\n",
    "            \n",
    "            text = msg['text']\n",
    "            \n",
    "            # Reemplazar referencias @ID con nombres de películas\n",
    "            for movie_id, movie_title in movie_mentions.items():\n",
    "                text = text.replace(f\"@{movie_id}\", f'\"{movie_title}\"')\n",
    "            \n",
    "            formatted.append(f\"{sender}: {text}\")\n",
    "        \n",
    "        return \"\\n\".join(formatted)\n",
    "    \n",
    "    def extract_ground_truth(self, conversation: Dict, \n",
    "                            from_message_idx: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extrae las películas que fueron sugeridas después del contexto\n",
    "        \n",
    "        Args:\n",
    "            conversation: diccionario con la conversación completa\n",
    "            from_message_idx: desde qué mensaje extraer ground truth\n",
    "            \n",
    "        Returns:\n",
    "            lista de títulos de películas sugeridas\n",
    "        \"\"\"\n",
    "        messages = conversation['messages'][from_message_idx:]\n",
    "        movie_mentions = conversation['movieMentions']\n",
    "        recommender_id = conversation['respondentWorkerId']\n",
    "        \n",
    "        # Extraer IDs de películas mencionadas por el RECOMMENDER después del contexto\n",
    "        mentioned_ids = set()\n",
    "        for msg in messages:\n",
    "            # Solo considerar mensajes del recommender\n",
    "            if msg['senderWorkerId'] == recommender_id:\n",
    "                text = msg['text']\n",
    "                ids = re.findall(r'@(\\d+)', text)\n",
    "                mentioned_ids.update(ids)\n",
    "        \n",
    "        # Filtrar solo las que fueron sugeridas (suggested=1)\n",
    "        ground_truth = []\n",
    "        \n",
    "        # Revisar en respondentQuestions\n",
    "        respondent_questions = conversation.get('respondentQuestions', {})\n",
    "        for movie_id in mentioned_ids:\n",
    "            if movie_id in respondent_questions:\n",
    "                if respondent_questions[movie_id].get('suggested', 0) == 1:\n",
    "                    title = movie_mentions.get(movie_id, '')\n",
    "                    if title:\n",
    "                        ground_truth.append(title)\n",
    "        \n",
    "        # También revisar initiatorQuestions (por si el seeker acepta algo)\n",
    "        initiator_questions = conversation.get('initiatorQuestions', {})\n",
    "        for movie_id in mentioned_ids:\n",
    "            if movie_id in initiator_questions:\n",
    "                if initiator_questions[movie_id].get('suggested', 0) == 1:\n",
    "                    title = movie_mentions.get(movie_id, '')\n",
    "                    if title and title not in ground_truth:\n",
    "                        ground_truth.append(title)\n",
    "        \n",
    "        return ground_truth\n",
    "    \n",
    "    def prepare_conversation_context(self, conversation: Dict, \n",
    "                                     context_ratio: float = 0.7) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Extrae el contexto de una conversación para dar al LLM\n",
    "        \n",
    "        Args:\n",
    "            conversation: diccionario con la conversación\n",
    "            context_ratio: qué porcentaje de mensajes usar como contexto\n",
    "            \n",
    "        Returns:\n",
    "            context_text: string con el contexto formateado\n",
    "            ground_truth: lista de títulos de películas que deberían recomendarse\n",
    "        \"\"\"\n",
    "        messages = conversation['messages']\n",
    "        \n",
    "        # Calcular cuántos mensajes usar como contexto\n",
    "        n_context_messages = max(1, int(len(messages) * context_ratio))\n",
    "        \n",
    "        # Formatear el contexto\n",
    "        context_text = self.format_conversation_context(\n",
    "            conversation,\n",
    "            n_context_messages\n",
    "        )\n",
    "        \n",
    "        # Extraer ground truth de los mensajes restantes\n",
    "        ground_truth = self.extract_ground_truth(conversation, n_context_messages)\n",
    "        \n",
    "        return context_text, ground_truth\n",
    "    \n",
    "    def get_evaluation_samples(self, n_samples: int = None, \n",
    "                              context_ratio: float = 0.7,\n",
    "                              min_ground_truth: int = 1) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retorna muestras para evaluación\n",
    "        \n",
    "        Args:\n",
    "            n_samples: cuántas muestras retornar (None = todas)\n",
    "            context_ratio: proporción de la conversación a usar como contexto\n",
    "            min_ground_truth: mínimo de películas en ground truth para incluir\n",
    "            \n",
    "        Returns:\n",
    "            list de dicts: [{'context': str, 'ground_truth': list, 'conv_id': str}, ...]\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        conversations = self.conversations[:n_samples] if n_samples else self.conversations\n",
    "        \n",
    "        for conv in conversations:\n",
    "            context, ground_truth = self.prepare_conversation_context(\n",
    "                conv, \n",
    "                context_ratio\n",
    "            )\n",
    "            \n",
    "            # Solo incluir si hay suficiente ground truth\n",
    "            if len(ground_truth) >= min_ground_truth:\n",
    "                samples.append({\n",
    "                    'context': context,\n",
    "                    'ground_truth': ground_truth,\n",
    "                    'conv_id': conv['conversationId'],\n",
    "                    'all_movies': list(conv['movieMentions'].values()),\n",
    "                    'n_messages_total': len(conv['messages']),\n",
    "                    'n_messages_context': int(len(conv['messages']) * context_ratio)\n",
    "                })\n",
    "        \n",
    "        print(f\"Prepared {len(samples)} samples with ground truth (min={min_ground_truth})\")\n",
    "        return samples\n",
    "\n",
    "# Test mejorado\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = ReDiALDataset('.', split='test')\n",
    "    samples = dataset.get_evaluation_samples(n_samples=5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE 1 - DETAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Conversation ID: {samples[0]['conv_id']}\")\n",
    "    print(f\"Total messages: {samples[0]['n_messages_total']}\")\n",
    "    print(f\"Context messages: {samples[0]['n_messages_context']}\")\n",
    "    print(\"\\nContext:\")\n",
    "    print(samples[0]['context'])\n",
    "    print(\"\\nGround Truth:\", samples[0]['ground_truth'])\n",
    "    print(\"\\nAll movies mentioned:\", samples[0]['all_movies'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total samples: {len(samples)}\")\n",
    "    gt_lengths = [len(s['ground_truth']) for s in samples]\n",
    "    print(f\"Avg ground truth size: {sum(gt_lengths)/len(gt_lengths):.2f}\")\n",
    "    print(f\"Min/Max ground truth: {min(gt_lengths)}/{max(gt_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90923487-f102-4e0b-a9f1-1460ec448ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀\n",
      "QUICK TEST - Gemma 2B with 100 samples\n",
      "🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀\n",
      "\n",
      "================================================================================\n",
      "STARTING EVALUATION\n",
      "================================================================================\n",
      "Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "Prompt strategy: zero_shot\n",
      "Samples: 100\n",
      "Context ratio: 0.7\n",
      "Temperature: 0.7\n",
      "8-bit quantization: False\n",
      "================================================================================\n",
      "\n",
      "[1/5] Loading dataset...\n",
      "Loading test data from ./test_data.jsonl...\n",
      "Loaded 1342 conversations\n",
      "Prepared 52 samples with ground truth (min=1)\n",
      "✓ Loaded 52 samples with ground truth\n",
      "\n",
      "[2/5] Loading model: Qwen/Qwen3-4B-Instruct-2507\n",
      "Loading Qwen/Qwen3-4B-Instruct-2507...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96edbca3d234aeca9ed128a379fb05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3f3414a87a495492607a6c57556647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4687d22b250b4c3583f841f451aa4b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a596a44cf441149b55b77a0b1834a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bce0f7be874f51b5edd7b67ed8dfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef213956900643e1b7a5f9842ab6c0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29ccd3411244050bd29147dffb2d862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb5384311b84acb96de275dea6850d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323d9cb59e514508baadcbb56db14c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4146493698224c9e84337427a1609c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b1e02c24f54402a7840a3fc14edd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a882be7ac71947d98452c6ee53c8fc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda!\n",
      "Model size: ~7.49 GB\n",
      "✓ Model loaded successfully\n",
      "\n",
      "[3/5] Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [12:12<00:00, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Evaluation complete\n",
      "  Successful: 52/52\n",
      "  Failed: 0/52\n",
      "\n",
      "[4/5] Computing aggregate metrics...\n",
      "\n",
      "[5/5] Saving results...\n",
      "✓ Results saved to ./results/Qwen3-4B-Instruct-2507_zero_shot_20251027_020746.json\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "recall@10           : 0.1587 ± 0.3326\n",
      "precision@10        : 0.0231 ± 0.0465\n",
      "ndcg@10             : 0.1094 ± 0.2441\n",
      "hit_rate@10         : 0.2115 ± 0.4084\n",
      "mrr                 : 0.1085 ± 0.2567\n",
      "latency             : 14.0647 ± 1.1320\n",
      "================================================================================\n",
      "\n",
      "Cleaning up memory...\n",
      "✓ Done\n",
      "\n",
      "\n",
      "✅ Quick test completed!\n",
      "Check the results folder for the output JSON file.\n",
      "\n",
      "Next steps:\n",
      "1. Review the results\n",
      "2. Run more experiments with different models/prompts\n",
      "3. Use run_multiple_experiments() for batch evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run_experiment.py\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def run_evaluation(\n",
    "    model_name: str,\n",
    "    dataset_path: str,\n",
    "    output_dir: str,\n",
    "    prompt_strategy: str = 'zero_shot',\n",
    "    n_samples: int = None,\n",
    "    load_in_8bit: bool = False,\n",
    "    context_ratio: float = 0.7,\n",
    "    temperature: float = 0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo de evaluación\n",
    "    \n",
    "    Args:\n",
    "        model_name: nombre del modelo en HuggingFace\n",
    "        dataset_path: ruta al dataset ReDial\n",
    "        output_dir: directorio donde guardar resultados\n",
    "        prompt_strategy: estrategia de prompting a usar\n",
    "        n_samples: cuántas muestras evaluar (None = todas)\n",
    "        load_in_8bit: usar cuantización 8-bit\n",
    "        context_ratio: proporción de conversación para contexto\n",
    "        temperature: temperatura de generación\n",
    "    \"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_short_name = model_name.split('/')[-1]\n",
    "    output_file = f\"{output_dir}/{model_short_name}_{prompt_strategy}_{timestamp}.json\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Prompt strategy: {prompt_strategy}\")\n",
    "    print(f\"Samples: {n_samples if n_samples else 'all'}\")\n",
    "    print(f\"Context ratio: {context_ratio}\")\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "    print(f\"8-bit quantization: {load_in_8bit}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Cargar dataset\n",
    "    print(\"\\n[1/5] Loading dataset...\")\n",
    "    dataset = ReDiALDataset(dataset_path, split='test')\n",
    "    samples = dataset.get_evaluation_samples(n_samples, context_ratio=context_ratio)\n",
    "    print(f\"✓ Loaded {len(samples)} samples with ground truth\")\n",
    "    \n",
    "    if len(samples) == 0:\n",
    "        print(\"✗ No samples with ground truth found!\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Cargar modelo\n",
    "    print(f\"\\n[2/5] Loading model: {model_name}\")\n",
    "    try:\n",
    "        llm = LLMRecommender(model_name, load_in_8bit=load_in_8bit)\n",
    "        print(\"✓ Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Seleccionar estrategia de prompt\n",
    "    if prompt_strategy not in PROMPT_STRATEGIES:\n",
    "        print(f\"✗ Unknown prompt strategy: {prompt_strategy}\")\n",
    "        print(f\"Available: {list(PROMPT_STRATEGIES.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    prompt_fn = PROMPT_STRATEGIES[prompt_strategy]\n",
    "    evaluator = RecommendationEvaluator(fuzzy_match_threshold=0.85)\n",
    "    \n",
    "    # 4. Evaluar\n",
    "    print(f\"\\n[3/5] Evaluating...\")\n",
    "    all_metrics = []\n",
    "    all_results = []\n",
    "    failed_samples = 0\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(samples, desc=\"Evaluating\")):\n",
    "        try:\n",
    "            context = sample['context']\n",
    "            ground_truth = sample['ground_truth']\n",
    "            \n",
    "            # Generar recomendaciones\n",
    "            response, latency = llm.generate_recommendations(\n",
    "                context, \n",
    "                prompt_fn,\n",
    "                max_new_tokens=250,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            # Extraer títulos\n",
    "            recommended = llm.extract_movie_titles(response, max_recommendations=10)\n",
    "            \n",
    "            # Calcular métricas\n",
    "            metrics = evaluator.evaluate_all(recommended, ground_truth, k_values=[5, 10])\n",
    "            metrics['latency'] = latency\n",
    "            metrics['n_recommended'] = len(recommended)\n",
    "            metrics['n_ground_truth'] = len(ground_truth)\n",
    "            \n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            # Guardar detalles (solo primeros 20 para no hacer JSON gigante)\n",
    "            if len(all_results) < 20:\n",
    "                all_results.append({\n",
    "                    'conv_id': sample['conv_id'],\n",
    "                    'context_preview': context[:300] + \"...\" if len(context) > 300 else context,\n",
    "                    'ground_truth': ground_truth,\n",
    "                    'recommended': recommended,\n",
    "                    'response_preview': response[:300] + \"...\" if len(response) > 300 else response,\n",
    "                    'metrics': metrics\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error in sample {i} (conv_id: {sample.get('conv_id', 'unknown')}): {e}\")\n",
    "            failed_samples += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✓ Evaluation complete\")\n",
    "    print(f\"  Successful: {len(all_metrics)}/{len(samples)}\")\n",
    "    print(f\"  Failed: {failed_samples}/{len(samples)}\")\n",
    "    \n",
    "    if len(all_metrics) == 0:\n",
    "        print(\"✗ No successful evaluations!\")\n",
    "        llm.clear_memory()\n",
    "        return None\n",
    "    \n",
    "    # 5. Agregar resultados\n",
    "    print(f\"\\n[4/5] Computing aggregate metrics...\")\n",
    "    aggregated = {}\n",
    "    \n",
    "    for key in all_metrics[0].keys():\n",
    "        values = [m[key] for m in all_metrics]\n",
    "        aggregated[f'{key}_mean'] = float(np.mean(values))\n",
    "        aggregated[f'{key}_std'] = float(np.std(values))\n",
    "        aggregated[f'{key}_median'] = float(np.median(values))\n",
    "        aggregated[f'{key}_min'] = float(np.min(values))\n",
    "        aggregated[f'{key}_max'] = float(np.max(values))\n",
    "    \n",
    "    # 6. Guardar resultados\n",
    "    print(f\"\\n[5/5] Saving results...\")\n",
    "    results = {\n",
    "        'experiment_info': {\n",
    "            'model_name': model_name,\n",
    "            'model_short_name': model_short_name,\n",
    "            'prompt_strategy': prompt_strategy,\n",
    "            'n_samples_requested': n_samples,\n",
    "            'n_samples_evaluated': len(samples),\n",
    "            'n_samples_successful': len(all_metrics),\n",
    "            'n_samples_failed': failed_samples,\n",
    "            'context_ratio': context_ratio,\n",
    "            'temperature': temperature,\n",
    "            'load_in_8bit': load_in_8bit,\n",
    "            'timestamp': timestamp\n",
    "        },\n",
    "        'aggregated_metrics': aggregated,\n",
    "        'sample_results': all_results\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to {output_file}\")\n",
    "    \n",
    "    # Mostrar resultados principales\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    key_metrics = ['recall@10', 'precision@10', 'ndcg@10', 'hit_rate@10', 'mrr', 'latency']\n",
    "    for metric in key_metrics:\n",
    "        mean_key = f'{metric}_mean'\n",
    "        std_key = f'{metric}_std'\n",
    "        if mean_key in aggregated:\n",
    "            print(f\"{metric:20s}: {aggregated[mean_key]:.4f} ± {aggregated[std_key]:.4f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Limpiar memoria\n",
    "    print(\"\\nCleaning up memory...\")\n",
    "    try:\n",
    "        llm.clear_memory()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"✓ Done\")\n",
    "    except:\n",
    "        print(\"Error limpiando memoria\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_multiple_experiments(experiments_config):\n",
    "    \"\"\"\n",
    "    Ejecuta múltiples experimentos en secuencia\n",
    "    \n",
    "    Args:\n",
    "        experiments_config: lista de dicts con configuraciones\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for i, config in enumerate(experiments_config):\n",
    "        print(f\"\\n\\n{'#'*80}\")\n",
    "        print(f\"EXPERIMENT {i+1}/{len(experiments_config)}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        result = run_evaluation(**config)\n",
    "        \n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "        \n",
    "        # Pequeña pausa entre experimentos\n",
    "        if i < len(experiments_config) - 1:\n",
    "            print(\"\\nWaiting 5 seconds before next experiment...\")\n",
    "            import time\n",
    "            time.sleep(5)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Crear directorio de resultados\n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "    \n",
    "    # EXPERIMENTO 1: Test rápido con modelo pequeño\n",
    "    print(\"\\n\" + \"🚀\"*40)\n",
    "    print(\"QUICK TEST - Gemma 2B with 100 samples\")\n",
    "    print(\"🚀\"*40)\n",
    "    \n",
    "    run_evaluation(\n",
    "        model_name=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "        dataset_path=\".\",\n",
    "        output_dir=\"./results\",\n",
    "        prompt_strategy='zero_shot',\n",
    "        n_samples=100,\n",
    "        load_in_8bit=False,\n",
    "        context_ratio=0.7,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\\n✅ Quick test completed!\")\n",
    "    print(\"Check the results folder for the output JSON file.\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Review the results\")\n",
    "    print(\"2. Run more experiments with different models/prompts\")\n",
    "    print(\"3. Use run_multiple_experiments() for batch evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc0d0b-ab0e-47a8-98a4-f8a0a0b35840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
