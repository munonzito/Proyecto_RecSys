# Evaluaci√≥n de LLMs Open-Source para Sistemas de Recomendaci√≥n Conversacional

Proyecto de investigaci√≥n para el curso IIC3633 - Sistemas Recomendadores (2025-2)

**Integrantes**: Mart√≠n Mu√±oz, Eduardo Soto, Randall Biermann

---

## üìã Descripci√≥n del Proyecto

Este proyecto eval√∫a y compara el rendimiento de Large Language Models (LLMs) open-source de √∫ltima generaci√≥n en tareas de recomendaci√≥n conversacional, utilizando el dataset ReDial. Se analizan diferentes arquitecturas de modelos, tama√±os de par√°metros y estrategias de prompting para determinar su efectividad en generar recomendaciones personalizadas de pel√≠culas basadas en conversaciones en lenguaje natural.

---

## üóÇÔ∏è Estructura del Repositorio

### Archivos Principales

#### `redial_dataset.py`
**Prop√≥sito**: Carga y preprocesa el dataset ReDial.

**Funcionalidades clave**:
- Carga conversaciones desde archivos JSONL (train/validation/test)
- Divide conversaciones en contexto y ground truth seg√∫n un ratio configurable
- Formatea conversaciones en texto legible para LLMs (User/Recommender)
- Extrae pel√≠culas recomendadas como ground truth
- Filtra muestras v√°lidas para evaluaci√≥n

**Uso**:
```python
from redial_dataset import ReDiALDataset

dataset = ReDiALDataset('.', split='test')
samples = dataset.get_evaluation_samples(n_samples=100, context_ratio=0.7)
```

---

#### `llm_recommender.py`
**Prop√≥sito**: Wrapper gen√©rico para cargar y usar cualquier LLM de HuggingFace.

**Funcionalidades clave**:
- Carga modelos de HuggingFace Transformers
- Soporta cuantizaci√≥n 8-bit para modelos grandes
- Genera recomendaciones dado un contexto conversacional
- Extrae t√≠tulos de pel√≠culas de las respuestas del modelo
- Gesti√≥n de memoria GPU

**Uso**:
```python
from llm_recommender import LLMRecommender

llm = LLMRecommender("google/gemma-2-2b-it", load_in_8bit=False)
response, latency = llm.generate_recommendations(context, prompt_fn)
movies = llm.extract_movie_titles(response)
```

---

#### `prompts.py`
**Prop√≥sito**: Define diferentes estrategias de prompting para los LLMs.

**Estrategias implementadas**:
- **Zero-shot**: Prompt b√°sico sin ejemplos
- **Few-shot**: Prompt con ejemplos de conversaciones y recomendaciones
- **Chain-of-thought**: Prompt que pide razonamiento paso a paso
- **Role-based**: Prompt con una persona espec√≠fica (experto en cine)
- **Structured**: Prompt que pide output en formato espec√≠fico

**Uso**:
```python
from prompts import PROMPT_STRATEGIES

prompt_fn = PROMPT_STRATEGIES['zero_shot']
formatted_prompt = prompt_fn(conversation_context)
```

---

#### `evaluator.py`
**Prop√≥sito**: Calcula m√©tricas de evaluaci√≥n para recomendaciones.

**M√©tricas implementadas**:
- **Recall@K**: Proporci√≥n de items relevantes que fueron recomendados
- **Precision@K**: Proporci√≥n de items recomendados que son relevantes
- **NDCG@K**: Normalized Discounted Cumulative Gain (considera ranking)
- **Hit Rate@K**: Si al menos 1 item relevante est√° en top-K
- **MRR**: Mean Reciprocal Rank del primer item relevante

**Caracter√≠sticas**:
- Fuzzy matching para manejar variaciones en nombres de pel√≠culas
- Normalizaci√≥n de t√≠tulos (remueve a√±os, puntuaci√≥n, etc.)
- Umbral de similitud configurable

**Uso**:
```python
from evaluator import RecommendationEvaluator

evaluator = RecommendationEvaluator(fuzzy_match_threshold=0.85)
metrics = evaluator.evaluate_all(recommended, ground_truth, k_values=[5, 10])
```

---

#### `run_experiment.py`
**Prop√≥sito**: Pipeline principal para ejecutar experimentos completos.

**Funcionalidades**:
- Orquesta el pipeline completo: carga datos ‚Üí carga modelo ‚Üí eval√∫a ‚Üí guarda resultados
- Calcula m√©tricas agregadas (media, std, mediana, min, max)
- Guarda resultados en JSON con metadata del experimento
- Maneja errores y memoria GPU
- Barra de progreso para seguimiento

**Uso**:
```python
from run_experiment import run_evaluation

run_evaluation(
    model_name="google/gemma-2-2b-it",
    dataset_path=".",
    output_dir="./results",
    prompt_strategy='zero_shot',
    n_samples=None,  # todas las muestras
    load_in_8bit=False,
    context_ratio=0.7,
    temperature=0.7
)
```

**Output**: Archivo JSON en `./results/` con:
- Informaci√≥n del experimento
- M√©tricas agregadas
- Ejemplos de resultados individuales

---

#### `experiments_midterm.py`
**Prop√≥sito**: Script para ejecutar batch de experimentos para el Midterm.

**Configuraci√≥n**:
Define lista de experimentos a ejecutar en secuencia:
- Diferentes modelos (Gemma, Llama, Qwen)
- Diferentes tama√±os (2B, 3B, 9B)
- Diferentes estrategias de prompting
- Configuraciones de cuantizaci√≥n

**Uso**:
```bash
python experiments_midterm.py
```

‚ö†Ô∏è **Advertencia**: Toma varias horas ejecutar todos los experimentos.

---

---

## üöÄ Gu√≠a de Uso R√°pido

### 1. Instalaci√≥n
```bash
# Instalar dependencias
pip install transformers torch
```
---

## üìä Formato de Resultados

Los resultados se guardan en `./results/` como archivos JSON:
```json
{
  "experiment_info": {
    "model_name": "google/gemma-2-2b-it",
    "prompt_strategy": "zero_shot",
    "n_samples_evaluated": 523,
    "context_ratio": 0.7,
    "timestamp": "20251027_123456"
  },
  "aggregated_metrics": {
    "recall@10_mean": 0.0288,
    "recall@10_std": 0.1523,
    "precision@10_mean": 0.0038,
    "ndcg@10_mean": 0.0135,
    "hit_rate@10_mean": 0.0385,
    "mrr_mean": 0.0092,
    "latency_mean": 9.05
  },
  "sample_results": [...]
}
```

---

## üî¨ Metodolog√≠a

### Pipeline de Evaluaci√≥n
```
1. PREPROCESAMIENTO
   ‚Üì
   - Cargar conversaciones ReDial
   - Dividir en contexto (70%) y ground truth (30%)
   - Filtrar conversaciones con ground truth v√°lido

2. GENERACI√ìN
   ‚Üì
   - Cargar LLM desde HuggingFace
   - Formatear prompt seg√∫n estrategia
   - Generar lista de 10 recomendaciones

3. EXTRACCI√ìN
   ‚Üì
   - Parsear respuesta del LLM
   - Extraer t√≠tulos de pel√≠culas
   - Limpiar formato

4. EVALUACI√ìN
   ‚Üì
   - Fuzzy matching con ground truth
   - Calcular m√©tricas (Recall, Precision, NDCG, etc.)
   - Medir latencia

5. AGREGACI√ìN
   ‚Üì
   - Promediar m√©tricas sobre todas las muestras
   - Calcular estad√≠sticas (mean, std, median)
   - Guardar resultados
```

### Context Ratio

El `context_ratio` define qu√© proporci√≥n de la conversaci√≥n se usa como contexto:
- **0.7** = 70% de mensajes como contexto, 30% para ground truth
- **0.6** = 60% de mensajes como contexto, 40% para ground truth

Ratios m√°s bajos ‚Üí m√°s ground truth disponible ‚Üí m√°s muestras v√°lidas

---

## üìà M√©tricas Clave

| M√©trica | Descripci√≥n | Interpretaci√≥n |
|---------|-------------|----------------|
| **Recall@10** | % de pel√≠culas relevantes que fueron recomendadas | Qu√© tan completo es el set de recomendaciones |
| **Precision@10** | % de recomendaciones que son relevantes | Qu√© tan preciso es el set de recomendaciones |
| **NDCG@10** | Calidad del ranking (penaliza items relevantes mal rankeados) | Qu√© tan bien ordenadas est√°n las recomendaciones |
| **Hit Rate@10** | % de conversaciones con al menos 1 acierto | √âxito b√°sico del sistema |
| **MRR** | Reciprocal rank del primer acierto | Qu√© tan arriba aparece el primer acierto |
| **Latency** | Tiempo de generaci√≥n (segundos) | Eficiencia computacional |

---